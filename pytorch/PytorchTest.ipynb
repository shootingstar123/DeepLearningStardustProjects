{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff046b2cf70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119],\n",
      "        [ 0.2710, -0.5435,  0.3462],\n",
      "        [-0.1188,  0.2937,  0.0803],\n",
      "        [-0.0707,  0.1601,  0.0285],\n",
      "        [ 0.2109, -0.2250, -0.0421],\n",
      "        [-0.0520,  0.0837, -0.0023],\n",
      "        [ 0.5047,  0.1797, -0.2150],\n",
      "        [-0.3487, -0.0968, -0.2490],\n",
      "        [-0.1850,  0.0276,  0.3442],\n",
      "        [ 0.3138, -0.5644,  0.3579],\n",
      "        [ 0.1613,  0.5476,  0.3811],\n",
      "        [-0.5260, -0.5489, -0.2785]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.5070, -0.0962,  0.2471],\n",
      "        [-0.2683,  0.5665, -0.2443],\n",
      "        [ 0.4330,  0.0068, -0.3042],\n",
      "        [ 0.2968, -0.3065,  0.1698],\n",
      "        [-0.1667, -0.0633, -0.5551],\n",
      "        [-0.2753,  0.3133, -0.1403],\n",
      "        [ 0.5751,  0.4628, -0.0270],\n",
      "        [-0.3854,  0.3516,  0.1792],\n",
      "        [-0.3732,  0.3750,  0.3505],\n",
      "        [ 0.5120, -0.3236, -0.0950],\n",
      "        [-0.0112,  0.0843, -0.4382],\n",
      "        [-0.4097,  0.3141, -0.1354]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2820,  0.0329,  0.1896,  0.1270,  0.2099,  0.2862, -0.5347,  0.2906,\n",
      "        -0.4059, -0.4356,  0.0351, -0.0984], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3391, -0.3344, -0.5133,  0.4202, -0.0856,  0.3247,  0.1856, -0.4329,\n",
      "         0.1160,  0.1387, -0.3866, -0.2739], requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)\n",
    "print(lstm.all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, 3) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [tensor([[-0.5525,  0.6355, -0.3968]]), tensor([[-0.6571, -1.6428,  0.9803]]), tensor([[-0.0421, -0.8206,  0.3133]]), tensor([[-1.1352,  0.3773, -0.2824]]), tensor([[-2.5667, -1.4303,  0.5009]])]\n"
     ]
    }
   ],
   "source": [
    "print('Inputs:',inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden: (tensor([[[ 0.5438, -0.4057,  1.1341]]]), tensor([[[-1.1115,  0.3501, -0.7703]]]))\n"
     ]
    }
   ],
   "source": [
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "print('Hidden:',hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1: tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('out1:',out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden2: (tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "print('hidden2:',hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ecd1bf63ca59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import jieba.posseg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import sys\n",
    "\n",
    "import gensim\n",
    "torch.manual_seed(2)\n",
    "# sys.stdout = open('1.log', 'a')\n",
    "sent='明天是荣耀运营十周年纪念日。' \\\n",
    "     '荣耀从两周年纪念日开始，' \\\n",
    "     '在每年的纪念日这天凌晨零点会开放一个新区。' \\\n",
    "     '第十版账号卡的销售从三个月前就已经开始。' \\\n",
    "     '在老区玩的不顺心的老玩家、准备进入荣耀的新手，都已经准备好了新区账号对这个日子翘首以盼。' \\\n",
    "    '陈果坐到了叶修旁边的机器，随手登录了她的逐烟霞。' \\\n",
    "     '其他九大区的玩家人气并没有因为第十区的新开而降低多少，' \\\n",
    "     '越老的区越是如此，实在是因为荣耀的一个账号想经营起来并不容易。' \\\n",
    "     '陈果的逐烟霞用了五年时间才在普通玩家中算是翘楚，哪舍得轻易抛弃。' \\\n",
    "     '更何况到最后大家都会冲着十大区的共同地图神之领域去。'\n",
    "words=jieba.posseg.cut(sent,HMM=True) #分词\n",
    "processword=[]\n",
    "tagword=[]\n",
    "for w in words:\n",
    "    processword.append(w.word)\n",
    "    tagword.append(w.flag)\n",
    "#词语和对应的词性做一一对应\n",
    "texts=[(processword,tagword)]\n",
    "\n",
    "#使用gensim构建本例的词汇表\n",
    "id2word=gensim.corpora.Dictionary([texts[0][0]])\n",
    "#每个词分配一个独特的ID\n",
    "word2id=id2word.token2id\n",
    "\n",
    "#使用gensim构建本例的词性表\n",
    "id2tag=gensim.corpora.Dictionary([texts[0][1]])\n",
    "#为每个词性分配ID\n",
    "tag2id=id2tag.token2id\n",
    "\n",
    "\n",
    "def sen2id(inputs):\n",
    "    return [word2id[word] for word in inputs]\n",
    "\n",
    "def tags2id(inputs):\n",
    "    return [tag2id[word] for word in inputs]\n",
    "#根据词汇表把文本输入转换成对应的词汇表的序号张量\n",
    "def formart_input(inputs):\n",
    "    return torch.tensor(sen2id(inputs),dtype=torch.long)\n",
    "\n",
    "#根据词性表把文本标注输入转换成对应的词汇标注的张量\n",
    "def formart_tag(inputs):\n",
    "    return torch.tensor(tags2id(inputs),dtype=torch.long)\n",
    "#定义网络结构\n",
    "class LSTMTagger(torch.nn.Module):\n",
    "    def __init__(self,embedding_dim,hidden_dim,voacb_size,target_size):\n",
    "        super(LSTMTagger,self).__init__()\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.voacb_size=voacb_size\n",
    "        self.target_size=target_size\n",
    "        # 使用Word2Vec预处理一下输入文本\n",
    "        self.embedding=nn.Embedding(self.voacb_size,self.embedding_dim)\n",
    "        #  LSTM 以 word_embeddings 作为输入, 输出维度为 hidden_dim 的隐状态值\n",
    "        self.lstm=nn.LSTM(self.embedding_dim,self.hidden_dim)\n",
    "        ## 线性层将隐状态空间映射到标注空间\n",
    "        self.out2tag=nn.Linear(self.hidden_dim,self.target_size)\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # 开始时刻, 没有隐状态\n",
    "        # 关于维度设置的详情,请参考 Pytorch 文档\n",
    "        # 各个维度的含义是 (Seguence, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        # 预处理文本转成稠密向量\n",
    "        embeds=self.embedding((inputs))\n",
    "        #根据文本的稠密向量训练网络\n",
    "        out,self.hidden=self.lstm(embeds.view(len(inputs),1,-1),self.hidden)\n",
    "        #做出预测\n",
    "        tag_space=self.out2tag(out.view(len(inputs),-1))\n",
    "        tags=F.log_softmax(tag_space,dim=1)\n",
    "        return tags\n",
    "\n",
    "\n",
    "model=LSTMTagger(10,10,len(word2id),len(tag2id))\n",
    "loss_function=nn.NLLLoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=0.1)\n",
    "#看看随机初始化网络的分析结果\n",
    "with torch.no_grad():\n",
    "    input_s=formart_input(texts[0][0])\n",
    "    print(input_s)\n",
    "    print(processword)\n",
    "    tag_s=model(input_s)\n",
    "    for i in range(tag_s.shape[0]):\n",
    "        print(tag_s[i])\n",
    "    # print(tag_s)\n",
    "for epoch in range(300):\n",
    "    # 再说明下, 实际情况下你不会训练300个周期, 此例中我们只是构造了一些假数据\n",
    "    for p ,t in texts:\n",
    "        # Step 1. 请记住 Pytorch 会累加梯度\n",
    "        # 每次训练前需要清空梯度值\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 此外还需要清空 LSTM 的隐状态\n",
    "        # 将其从上个实例的历史中分离出来\n",
    "        # 重新初始化隐藏层数据，避免受之前运行代码的干扰,如果不重新初始化，会有报错。\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. 准备网络输入, 将其变为词索引的Tensor 类型数据\n",
    "        sentence_in=formart_input(p)\n",
    "        tags_in=formart_tag(t)\n",
    "\n",
    "        # Step 3. 前向传播\n",
    "        tag_s=model(sentence_in)\n",
    "\n",
    "        # Step 4. 计算损失和梯度值, 通过调用 optimizer.step() 来更新梯度\n",
    "        loss=loss_function(tag_s,tags_in)\n",
    "        loss.backward()\n",
    "        print('Loss:',loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "#看看训练后的结果\n",
    "with torch.no_grad():\n",
    "    input_s=formart_input(texts[0][0])\n",
    "    tag_s=model(input_s)\n",
    "    for i in range(tag_s.shape[0]):\n",
    "        print(tag_s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
